# Umbra Database

## Overview

Umbra is an experimental compiled database system where data is directly embedded in C code, compiled into shared objects, and loaded dynamically. Unlike traditional databases that parse data at runtime, Umbra's hopes to achieve exceptional performance through zero-copy access and compiler optimizations.  

Think of the data itself as a DLL (Shared Object .so in reality), and the system loads the table dynamically without needing to parse the structure.  Queries are similarly compiled to from SQL-like statements to C code that can interact with and read the data.  Technically, you could use `git` as a replication system, or even `rsync`.  

For this project I used my original (mostly abandoned) prototype and worked with claude.ai to expand the idea into something approaching a minimum viable product. See branch `prototype` for the original idea which was just the code that compiles to the `.so` files and query tool and dynamic loading that I'd written by hand.  It took me a little less than a week to build a CLI version that worked well enough to post.

I probably wouldn't use this in production just yet - but you could easily pull out the data files and build kernels to have a large/fast/immutable database embedded in your system that could be easily updated.

## Quickstart

### Basic Build Commands
```bash
# Build library, CLI executable and tests
make

# Build and run all tests
make test

# Build and run specific tests
make test_create_table
make test_crud
make test_index

# Clean build artifacts
make clean 

# View detailed build information
make info
```

### Installation?
 
I recommend just testing it out and NOT installing it, this is just an experiment... just build and run it for fun, I haven't event tried installing it.

## Required Ubuntu Packages

To build and run the Umbra Database on Ubuntu, you'll need to install the following packages:

```bash
# Essential build tools
sudo apt install build-essential  # Includes gcc, make, and basic build tools

# Required libraries
sudo apt install libreadline-dev  # For CLI interactive mode
sudo apt install uuid-dev         # For UUID generation

# Debugging tools (optional but recommended)
sudo apt install gdb              # For debugging
sudo apt install valgrind         # For memory leak detection

# Other potential dependencies
sudo apt install git              # For version control and data distribution
```

## Basic Usage (Based on Makefile Structure)

After building, you can run the database using:
```bash
make
./build/bin/umbra
```
You can then do some basic CREATE TABLE from SQL, SELECTS, INSERTS, UPDATES, DELETES, and CREATE INDEX calls.
Then you can go into the test database subdirectory and view the source and compiled binaries.

## Key Features

- **Compiled Data Pages**: Data definitions are converted to C structs and compiled into shared objects
- **Dynamic Loading**: Pages are loaded on-demand, minimizing memory footprint
- **Zero-Copy Access**: Direct memory access without parsing or deserialization
- **Git Integration**: Database distribution and replication via git
- **Compiler Optimizations**: Leverages C compiler for memory alignment and access patterns
- **Designed for Historical Data**: Optimized for data that changes infrequently

## Technical Approach

The direct embedding of data in C code and compilation to shared objects is a fundamental architectural choice for Umbra. This approach offers significant performance advantages through zero-copy access, compiler optimization, and elimination of parsing overhead.



### Embedded Data Arrays

Data is stored in header files that are included directly within C array initializations:

```c
/* Header file containing struct array declaration */
struct Customers CustomersData_0[] = {
    /*BEGIN Customers DATA*/
    #include "../data/addressBookData.0.dat.h"  /* Include data directly in array! */
    /*END Customers DATA*/
    NULL /* NULL terminator for easy traversal */
};
```

The included .dat.h file contains pure data entries formatted as C struct initializers:

```c
/*This file autogenerated, do not edit*/
{"0","John Q Public0","Chenoa", readOnly ,{0.1,0,0,0,0}},
{"1","John Q Public1","Chenoa", readOnly ,{0.1,0,0,0,0}},
{"2","John Q Public2","Chenoa", readOnly ,{0.1,0,0,0,0}},
/* ... more data entries ... */
```

### Paged Structure

Data is divided into pages with predictable naming patterns:
- `addressBookData.0.dat.h`
- `addressBookData.1.dat.h`
- `addressBookData.2.dat.h`

Each page is compiled into a separate shared object (.so) file:
- `addressBookData.0.so`
- `addressBookData.1.so`
- etc.

### Accessor Functions

Each compiled data page exports standard accessor functions:

```c
int count(void); /* Returns the number of records in the page */
struct Customers *read(int pos); /* Returns a pointer to a specific record */
```

These functions are dynamically loaded with the data, enabling a consistent interface across all pages.

## Advantages

### 1. Zero-Copy Access

Data is directly accessible in memory without parsing or deserialization:

```c
// Access data directly through pointers - no copying required
struct Customers *record = page.read(position);
printf("Name: %s\n", record->name);
```

### 2. Compiler Optimization

The C compiler optimizes data layout and access patterns:
- Memory alignment optimizations
- Potential for compile-time validation of data
- Efficient memory layout based on compiler knowledge

### 3. Type Safety

Data inherits C's type system, ensuring consistency:
- Type errors caught at compile time
- No runtime parsing errors
- Consistent memory representation

### 4. Dynamic Loading

Only needed data pages are loaded into memory:
```c
struct addressBookDynamicPage page = loadPage(pageNumber);
if(page.error == 0) {
    // Use the page data
    int recordCount = page.count();
    // ...
    unload_page(page); // Release when done
}
```

### 5. Zero Serialization/Deserialization Overhead

No conversion between external and internal formats:
- No parsing time
- No formatting time
- No memory overhead for conversion

## CLI Components

The Command Line Interface (CLI) provides a way to interact with the Umbra database system. It consists of:

1. **Interactive Mode (REPL)**
   - Accept SQL commands interactively
   - Parse and execute commands
   - Display results in formatted tables
   - Support command history and editing

2. **Result Formatting**
   - Format query results as ASCII tables
   - Support CSV and JSON output formats
   - Handle large result sets with pagination

3. **Special Commands**
   - `.tables` to list tables
   - `.schema` to show table structure
   - `.exit` and `.help` commands

### CLI Files Overview

The CLI implementation is divided into several components:

| File | Description |
|------|-------------|
| `cli_main.c` | Main entry point for CLI application |
| `interactive_mode.h/c` | REPL for interactive SQL commands |
| `command_mode.h/c` | Handles command-line SQL execution |
| `result_formatter.h/c` | Formats query results in different formats |
| `cli_commands.h/c` | Implements special CLI commands (.help, .exit, etc.) |
| `command_history.h/c` | Manages command history for interactive mode |

### Using the CLI

```
Usage: umbra [options] [database_path]

Options:
  -c, --command <sql>    Execute SQL command and exit
  -f, --file <file>      Execute SQL from file and exit
  -o, --output <format>  Output format (table, csv, json)
  -h, --help             Show this help message
  -v, --version          Show version information
```

### Interactive Mode

In interactive mode, you can:
- Type SQL commands that end with semicolons
- Use special dot commands (like `.help` or `.tables`)
- Navigate command history with up/down arrows
- Exit with `.exit`, `.quit`, or `EXIT` commands

### Result Formatting

The CLI supports multiple output formats:

1. **Table Format** (default)
   ```
   +----------+----------+
   | column1  | column2  |
   +----------+----------+
   | value1   | value2   |
   +----------+----------+
   1 row returned
   ```

2. **CSV Format**
   ```
   column1,column2
   value1,value2
   ```

3. **JSON Format**
   ```json
   {
     "rows": 1,
     "columns": [
       { "name": "column1", "type": "varchar" },
       { "name": "column2", "type": "int" }
     ],
     "data": [
       { "column1": "value1", "column2": 42 }
     ]
   }
   ```

## Indexing System

The Umbra database includes a sophisticated indexing system that follows the same core philosophy as the rest of the database: everything is compiled to native code. This section describes the architecture and implementation of this system.

### Overview

The indexing system accelerates queries by creating specialized data structures that allow for rapid lookup of records based on column values. Rather than scanning entire tables, queries can use these indices to jump directly to relevant records. Umbra supports two primary index types:

- **B-tree indices** - Optimized for range queries (`>, <, >=, <=`)
- **Hash indices** - Optimized for equality queries (`=`)

Like data pages, indices are compiled into shared objects (`.so` files) that can be dynamically loaded at runtime, maintaining Umbra's zero-copy architecture and leveraging compiler optimizations.

### Architecture

The indexing system consists of several interconnected components:

1. **Index Definition** - Data structures and metadata that describe indices
2. **Index Manager** - Handles the lifecycle of indices (creation, loading, deletion)
3. **Index Generator** - Creates C source code for indices
4. **Index Compiler** - Compiles generated source code into shared objects
5. **Index Types** - Implementations of specific index structures (B-tree, Hash)

Each index is associated with a specific table column and has a consistent interface, allowing the query system to use them transparently regardless of the underlying implementation.

### Index Creation Process

When a `CREATE INDEX` statement is executed:

1. The statement is parsed to extract the table name, column name, and index type
2. The Index Manager verifies that the column exists and isn't already indexed
3. The Index Generator creates C source code for the index based on the column's data type
4. The Index Compiler creates a compilation script and compiles the index into a shared object
5. The Index Manager records metadata about the index for future use

```
┌─ CREATE INDEX ─┐          ┌─ Index Manager ─┐         ┌─ Index Generator ─┐
│ SQL Statement  │───────▶ │ Validation &     │───────▶│ Create C source    │
└────────────────┘          │ Metadata         │         │ code for index     │
                            └─────────┬────────┘         └────────┬───────────┘
                                      │                           │
                                      │                           ▼
                                      │                  ┌─ Index Compiler ─┐
                                      │                  │ Compile index to │
                                      │                  │ shared object     │
                                      │                  └────────┬──────────┘
                                      ▼                           │
                              ┌─ Final Index ─┐                   │
                              │ Compiled .so  │◀────────────────── 
                              │ + Metadata    │
                              └───────────────┘
```

### Implementation Details

#### Index Types

Umbra supports two index types with different performance characteristics:

1. **B-tree Index**
   - Balanced tree structure (order = 5)
   - Supports range queries efficiently
   - Keys are sorted, allowing for sequential access
   - Implemented in `btree_index.h/c`

2. **Hash Index**
   - Hash table with collision resolution
   - Very fast equality lookups (O(1) average case)
   - Does not support range queries
   - Implemented in `hash_index.h/c`

#### Index Structure

Each index consists of:

- **Metadata** - Information about the index (name, table, column, type)
- **Key Information** - Data type, size, and offset of the indexed column
- **Index Structure** - The actual data structure (B-tree nodes or hash buckets)
- **Access Functions** - Standardized functions to search the index

```c
// Example B-tree index structure
typedef struct {
    BTreeNode* root;           // Root node of B-tree
    IndexMetadata metadata;    // Index metadata
    int (*compare)(const void*, const void*); // Comparison function
} BTreeIndex;

// Example Hash index structure
typedef struct {
    HashBucket* buckets;       // Array of buckets
    int bucket_count;          // Number of buckets
    IndexMetadata metadata;    // Index metadata
    unsigned int (*hash)(const void*); // Hash function
} HashIndex;
```

#### Generated Code

Indices are generated as C code that is compiled, similar to data pages. For each index, the system generates:

1. **Header file** - Declares the interface functions
2. **Source file** - Implements the index structure and search algorithms
3. **Compilation script** - Automates the compilation process

The generated code is tailored to the specific column being indexed, including specialized comparison functions based on the column's data type.

### Using Indices

Indices are created with the `CREATE INDEX` SQL statement:

```sql
-- Create a B-tree index on the 'age' column
CREATE INDEX ON Customers (age) USING BTREE;

-- Create a hash index on the 'email' column
CREATE INDEX ON Customers (email) USING HASH;
```

Once created, indices are automatically used by the query optimizer when executing queries. The optimizer considers available indices and selects the most appropriate one based on the query conditions.

### Performance Considerations

- **B-tree indices** are best for:
  - Range queries (`WHERE age BETWEEN 20 AND 30`)
  - Ordered data access (`ORDER BY column`)
  - Equality queries when the key is not unique

- **Hash indices** are best for:
  - Exact match queries (`WHERE email = 'user@example.com'`)
  - Primary key or unique constraint lookups
  - High cardinality columns (many unique values)

# Query Kernel System

## Overview

The Query Kernel System is a core innovation of the Umbra database, implementing the "code as data" approach by compiling queries into native machine code. This system converts SQL queries into C code, compiles them into shared objects, and then executes these compiled kernels against the embedded data pages.

## Components

### 1. Kernel Generator

The kernel generator transforms SQL queries (AST) into executable C code:

- **File:** `src/kernel/kernel_generator.h/c`
- **Purpose:** Generates C code from SQL AST for query execution
- **Features:**
  - Creates unique kernel names based on query hash
  - Generates complete C source files with includes, structures, and functions
  - Supports SELECT queries with projection and filtering
  - Specialized handling for COUNT(*) queries

### 2. Filter Generator

The filter generator converts WHERE clauses into C predicates:

- **File:** `src/kernel/filter_generator.h/c`
- **Purpose:** Transforms SQL conditions into C expressions
- **Features:**
  - Supports comparison operators (=, !=, <, >, <=, >=)
  - Handles logical operators (AND, OR)
  - Provides type-specific comparisons (e.g., string vs. numeric)
  - Validates expressions against schema definitions

### 3. Projection Generator

The projection generator manages column selection:

- **File:** `src/kernel/projection_generator.h/c`
- **Purpose:** Generates code for selecting and formatting result columns
- **Features:**
  - Creates custom result structures for projections
  - Handles SELECT * by mapping to full schema
  - Generates assignment code for selected columns
  - Special handling for string types

### 4. Kernel Compiler

The kernel compiler builds shared objects from generated code:

- **File:** `src/kernel/kernel_compiler.h/c`
- **Purpose:** Compiles generated C code into executable shared objects
- **Features:**
  - Creates compilation scripts
  - Manages dependencies and include paths
  - Executes compilation process
  - Caches compiled objects for reuse

### 5. Kernel Loader

The kernel loader dynamically loads and executes compiled kernels:

- **File:** `src/kernel/kernel_loader.h/c`
- **Purpose:** Loads and executes compiled query kernels
- **Features:**
  - Dynamically loads shared objects
  - Maps to standardized kernel function interface
  - Executes kernels against data pages
  - Manages kernel lifecycle

## Workflow

The query kernel system follows this sequence:

1. **Parse** - SQL query is parsed into an AST
2. **Generate** - AST is converted to C code via the kernel generator
   - Filter conditions are generated for WHERE clauses
   - Projection structures and assignments are created for SELECT lists
3. **Compile** - Generated C code is compiled into a shared object
4. **Load** - Compiled kernel is dynamically loaded
5. **Execute** - Loaded kernel is executed against data pages
6. **Results** - Query results are returned in the projection format

## Example

For a query like:

```sql
SELECT name, email FROM Customers WHERE age > 30 AND active = true
```

The system will:

1. Generate a C kernel with:
   - A result structure containing `name` and `email` fields
   - Filter code that checks `age > 30 && active == true`
   - Projection code that copies `name` and `email` to results
   
2. Compile this code to `kernel_XXXX_Customers.so`

3. Load and execute the kernel against Customers data pages

The kernel looks similar to:

```c
/* Generated kernel for Customers table */
/* Kernel: kernel_12ab34cd */

#include <stdio.h>
#include <stdlib.h>
#include <string.h>
#include <stdbool.h>
#include "../tables/Customers/Customers.h"

/* Result structure for projected columns */
typedef struct {
    char name[64];
    char email[128];
} KernelResult;

/* Execute compiled query kernel */
int kernel_12ab34cd(Customers* data, int count, KernelResult* results, int max_results) {
    int result_count = 0;

    for (int i = 0; i < count && result_count < max_results; i++) {
        Customers* record = &data[i];
        
        /* WHERE clause */
        if (!(record->age > 30 && record->active == true)) {
            continue;
        }
        
        /* Project selected columns */
        strcpy(results[result_count].name, record->name);
        strcpy(results[result_count].email, record->email);
        result_count++;
    }
    
    return result_count;
}
```

### Shared Object Loading

The shared object loading system (`so_loader.h/c`) provides the foundation for dynamically loading compiled data pages:

```c
/**
 * @struct LoadedLibrary
 * @brief Represents a dynamically loaded shared object
 */
typedef struct {
    void* handle;               /**< Handle to the loaded library */
    char path[1024];            /**< Path to the library file */
    bool loaded;                /**< Whether the library is loaded */
} LoadedLibrary;
```

Key functions:
- `load_library()`: Loads a shared object file
- `unload_library()`: Unloads a previously loaded shared object
- `get_function()`: Retrieves a function pointer from a loaded library

### Page Management

The page management system (`page_manager.h/c`) builds on the shared object loader to provide a higher-level interface for working with data pages:

```c
/**
 * @struct LoadedPage
 * @brief Represents a loaded data page
 */
typedef struct {
    LoadedLibrary library;      /**< Loaded shared object */
    char table_name[64];        /**< Table name */
    int page_number;            /**< Page number */
    int (*count)(void);         /**< Function pointer to count() */
    void* (*read)(int);         /**< Function pointer to read() */
    bool valid;                 /**< Whether the page is valid */
} LoadedPage;
```

Key functions:
- `load_page()`: Loads a data page
- `unload_page()`: Unloads a previously loaded page
- `get_page_count()`: Gets the count of records in a loaded page
- `read_record()`: Reads a record from a loaded page

### Record Access

The record access system (`record_access.h/c`) provides cursor-based access to records across pages:

```c
/**
 * @struct TableCursor
 * @brief Cursor for iterating through table records
 */
typedef struct {
    char base_dir[1024];         /**< Base directory for database */
    char table_name[64];         /**< Table name */
    int current_page;            /**< Current page number */
    int current_position;        /**< Current position in page */
    LoadedPage loaded_page;      /**< Currently loaded page */
    int total_pages;             /**< Total number of pages */
    bool initialized;            /**< Whether cursor is initialized */
    bool at_end;                 /**< Whether cursor is at the end */
} TableCursor;
```

Key functions:
- `init_cursor()`: Initializes a table cursor
- `next_record()`: Moves the cursor to the next record
- `get_current_record()`: Gets the current record from the cursor
- `get_field_from_record()`: Gets a field from a record by name
- `get_field_by_index()`: Gets a field from a record by index

### Error Handling

The error handling system (`error_handler.h/c`) provides centralized error reporting:

```c
/**
 * @brief Set an error message
 */
void set_error(const char* format, ...);

/**
 * @brief Get the last error message
 */
const char* get_last_error(void);
```

## Data Embedding Technique

### How Data is Embedded

Data is stored in header files that are included directly within C array initializations, creating a seamless integration between code and data:

```c
/* Header file containing struct array declaration */
struct Customers CustomersData_0[] = {
    /*BEGIN Customers DATA*/
    #include "../data/addressBookData.0.dat.h"  /* Include data directly in array! */
    /*END Customers DATA*/
};
```

The included `.dat.h` file contains pure data entries formatted as C struct initializers:

```c
/*This file autogenerated, do not edit*/
{"0","John Q Public0","Chenoa", readOnly, {0.1,0,0,0,0}},
{"1","John Q Public1","Chenoa", readOnly, {0.1,0,0,0,0}},
{"2","John Q Public2","Chenoa", readOnly, {0.1,0,0,0,0}},
/* ... more data entries ... */
```

### Paged Architecture

Data is divided into pages with predictable naming patterns:
- `addressBookData.0.dat.h`
- `addressBookData.1.dat.h`
- `addressBookData.2.dat.h`

Each page is compiled into a separate shared object (.so) file which can be loaded independently:
- `addressBookData.0.so`
- `addressBookData.1.so`

This enables dynamic loading of only the required data pages, reducing memory footprint.

## Key Components

### 1. Page Template System (`page_template.h`)

The page template system defines the structure for generated files:

- **DATA_PAGE_HEADER_TEMPLATE**: Template for data page header files
- **ACCESSOR_SOURCE_TEMPLATE**: Template for C source files with accessor functions
- **COMPILE_SCRIPT_TEMPLATE**: Template for compilation scripts

These templates ensure consistency across all generated files.

### 2. Page Generator (`page_generator.h/c`)

The page generator creates the necessary files for each data page:

- **generate_data_page()**: Creates a new data page for a table
- **add_record_to_page()**: Adds a record to an existing page
- **is_page_full()**: Checks if a page has reached its maximum capacity
- **get_page_record_count()**: Counts records in a page
- **recompile_data_page()**: Recompiles a page after modifications

The page generator ensures data is properly formatted as C struct initializers.

### 3. Page Splitter (`page_splitter.h/c`)

The page splitter manages page size constraints:

- **check_page_split()**: Determines if a page needs to be split
- **split_page()**: Divides a full page into two pages
- **get_table_page_info()**: Retrieves metadata about pages in a table
- **find_best_page_for_insert()**: Identifies the optimal page for a new record

This component maintains appropriate page sizes and redistributes data when necessary.

### 4. Accessor Generator (`accessor_generator.h/c`)

The accessor generator creates standard interface functions for each page:

- **generate_accessor_file()**: Creates a source file with standard accessor functions
- **generate_filtered_accessor()**: Creates accessors with filtering capabilities (WHERE clause)
- **generate_projection_accessor()**: Creates accessors that return only selected columns

These accessors provide a consistent API across all data pages:

```c
int count(void); /* Returns the number of records in the page */
struct Customers *read(int pos); /* Returns a pointer to a specific record */
```

### 5. Compilation Scripts (`compile_scripts.h/c`)

The compilation scripts system manages the build process:

- **generate_compilation_script()**: Creates a script to compile a specific page
- **generate_filtered_compilation_script()**: Creates scripts for filtered accessors
- **generate_table_makefile()**: Creates a makefile for all pages in a table

These scripts automate the compilation of data pages into shared objects.

## Workflow

### Data Page Generation

1. Schema definition is translated into C struct definitions
2. Empty data page files are created with appropriate headers
3. Accessor function files are generated for the page
4. Compilation scripts are created

### Record Insertion

1. Records are added to the appropriate page file as C struct initializers
2. If a page becomes full, it may be split into two pages
3. After modifications, pages are recompiled into shared objects

### Query Processing

1. Required data pages are dynamically loaded
2. Query-specific filter and projection functions may be generated
3. Query execution occurs directly on the in-memory data structures
4. Results are returned without serialization/deserialization

## Implementation Details

### Page Generation Process

```
Schema Definition -> C Struct Definition -> Empty Page Files -> Accessor Functions -> Compilation
```

When generating a page, the system:
1. Creates the directory structure if needed
2. Generates an empty data page header file
3. Creates accessor function source file from the template
4. Generates a compilation script
5. (Optionally) Compiles the page into a shared object

### Record Format

Records are stored as C struct initializers, with appropriate type conversion:
- Strings are quoted: `"John Doe"`
- Numbers are unquoted: `42`, `3.14`
- Booleans are lowercase: `true`, `false`
- NULL values are converted to appropriate defaults

### Dynamic Loading

Pages are loaded on-demand using the standard dynamic library loading mechanism (`dlopen`, `dlsym`). This allows:
- Loading only needed data
- Unloading unused data
- Efficient memory usage


# Umbra SQL Parser Documentation

## Overview

The SQL Parser is a critical component of the Umbra database system, responsible for parsing and validating SQL statements before they're executed. This component transforms the textual SQL queries into an Abstract Syntax Tree (AST) representation that can be used to generate compiled C code for execution.

## Architecture

The SQL Parser follows a modular design with specific components for different statement types. This organization allows for clean separation of concerns while keeping each file under 250 lines for better maintainability.

### Key Components

1. **Lexer**: Tokenizes SQL input with reference counting for memory management
2. **AST Definitions**: Defines the Abstract Syntax Tree structures
3. **Statement Parsers**: Specialized parsers for each SQL statement type (SELECT, INSERT, UPDATE, DELETE)
4. **Common Utilities**: Shared functionality across parsers

## File Structure

```
src/parser/
├── lexer.h/c              - Tokenizes SQL input with reference counting
├── ast.h/c                - Abstract Syntax Tree definitions and utilities
├── parser_common.h/c      - Shared parser utilities
├── select_parser.h/c      - Parser for SELECT statements
├── insert_parser.h/c      - Parser for INSERT statements
├── update_parser.h/c      - Parser for UPDATE statements
└── delete_parser.h/c      - Parser for DELETE statements
```

## Component Details

### Lexer (`lexer.h/c`)

The lexer tokenizes SQL input strings into tokens with reference counting for efficient memory management.

Key features:
- Token reference counting for memory safety
- Support for SQL keywords, identifiers, literals, and operators
- Position tracking for error reporting
- Peek functionality for lookahead parsing

```c
// Example token creation with reference counting
Token token = token_create(TOKEN_IDENTIFIER, "column_name", line, column);
token_ref(&token);   // Increment reference count
token_unref(&token); // Decrement reference count, free if zero
```

### Abstract Syntax Tree (`ast.h/c`)

The AST defines structures to represent parsed SQL statements in a hierarchical form.

Key components:
- Expression nodes (literals, column references, operators)
- Statement nodes (SELECT, INSERT, UPDATE, DELETE)
- Support functions for creating and freeing AST nodes

```c
// Example AST node creation
Expression* expr = create_expression(AST_COLUMN_REF);
expr->data.column = *create_column_ref("table", "column", NULL);

// Statement creation
SelectStatement* select = create_select_statement();
add_select_expression(select, expr);
```

### Statement Parsers

Each statement type has a dedicated parser:

#### SELECT Parser (`select_parser.h/c`)
- Parses SELECT statements with columns, FROM, and WHERE clauses
- Handles expressions, comparisons, and logical operators
- Supports function calls and nested expressions

#### INSERT Parser (`insert_parser.h/c`)
- Parses INSERT statements with table name, optional column list, and values
- Validates statement structure against schema
- Reports detailed validation errors

#### UPDATE Parser (`update_parser.h/c`)
- Parses UPDATE statements with table name, SET clauses, and optional WHERE
- Handles multiple SET operations in a single statement
- Validates against schema to prevent primary key updates

#### DELETE Parser (`delete_parser.h/c`)
- Parses DELETE statements with table name and optional WHERE clause
- Issues warnings for DELETE without WHERE (potentially dangerous)
- Validates statement structure

### Common Parser Utilities (`parser_common.h/c`)

Provides shared functionality for all parsers:
- Token matching and expectation checking
- Error reporting and tracking
- Expression parsing (used by all statement parsers)
- Token consumption and resource cleanup

```c
// Example usage
if (!expect(parser, TOKEN_FROM, "Expected FROM after SELECT list")) {
    return NULL;
}

// Parse expression 
Expression* where_clause = parse_expression(parser);
```

## Integration with Umbra

The SQL Parser integrates with the Umbra database system in the following ways:

1. **Schema Validation**: Parsed statements are validated against the table schema
2. **Code Generation**: The AST is used to generate optimized C code for query execution
3. **Dynamic Loading**: Generated code interacts with the dynamically loaded data pages
4. **Transactions**: Modifications from INSERT/UPDATE/DELETE are tracked for commits

## Memory Management

The SQL Parser implements careful reference counting to avoid memory leaks:
- Tokens use reference counting with `token_ref()` and `token_unref()`
- AST nodes have explicit `free_*` functions to clean up hierarchies
- Parser state properly cleans up tokens during error conditions

## Example Parsing Flow

```
Input: "SELECT name, email FROM Customers WHERE age > 30"
  │
  ▼
Lexer tokenizes into: [SELECT, name, COMMA, email, FROM, Customers, WHERE, age, GREATER, 30]
  │
  ▼
Parser creates:
  SelectStatement {
    select_list: [ColumnRef{name}, ColumnRef{email}],
    from_table: TableRef{Customers},
    where_clause: BinaryOp{OP_GREATER, ColumnRef{age}, Literal{30}}
  }
  │
  ▼
Statement is validated against schema
  │
  ▼
AST is used for code generation
```

### Query System Components

Umbra's query system compiles SQL statements into native code for execution:

#### Query Execution Flow

1. **SQL Parsing**: The query is parsed into an Abstract Syntax Tree (AST)
2. **Kernel Generation**: C code is generated from the AST to implement the query
3. **Kernel Compilation**: Generated code is compiled into a shared object
4. **Kernel Loading**: The compiled kernel is dynamically loaded
5. **Query Execution**: The kernel is executed against the relevant data pages

#### Key Query Components

- **Table Scan**: The `table_scan.h/c` module manages scanning table data across pages
- **Query Executor**: The `query_executor.h/c` handles execution of all query types
- **Select Executor**: The `select_executor.h/c` implements SELECT statement execution
- **Modification Executors**: Separate modules handle INSERT, UPDATE, and DELETE operations

### Implementation Details

#### Data Page Management

Data is divided into pages with predictable naming patterns:
- `addressBookData.0.dat.h`
- `addressBookData.1.dat.h`
- etc.

Each page is compiled into a separate shared object file for dynamic loading:
- `addressBookData.0.so`
- `addressBookData.1.so`
- etc.

#### Accessor Interface

Each compiled data page exports standard accessor functions:

```c
int count(void); /* Returns the number of records in the page */
struct Customers *read(int pos); /* Returns a pointer to a specific record */
```

These functions provide a consistent interface across all data pages.

#### Dynamic Loading

Only needed data pages are loaded into memory:

```c
struct addressBookDynamicPage page = loadPage(pageNumber);
if(page.error == 0) {
    // Use the page data
    int recordCount = page.count();
    // ...
    unload_page(page); // Release when done
}
```

### Schema Management

The schema system converts SQL table definitions into C struct definitions:

```c
// SQL: CREATE TABLE Customers (name VARCHAR(50), email VARCHAR(100))
// Generates:
typedef struct {
    char _uuid[37];    // Auto-added primary key
    char name[51];     // VARCHAR(50) + null terminator
    char email[101];   // VARCHAR(100) + null terminator
} Customers;
```

Key components:
- `schema_parser.h/c`: Parses CREATE TABLE statements
- `schema_generator.h/c`: Generates C struct definitions
- `type_system.h/c`: Handles data type conversions and validation

### Directory Structure

Umbra maintains an organized directory structure for each table:

```
/base_dir
  /tables
    /table_name
      /metadata
      /data       <- Contains .dat.h files with actual data
      /src        <- Source files for compiled data pages
  /compiled       <- Compiled .so files
  /permissions
```

Key components:
- `directory_manager.h/c`: Creates and manages directory structures
- `metadata.h/c`: Stores and retrieves table metadata

### Data Page Generation

Data is stored in header files included directly in C array initializations:

```c
// Source file with array declaration
static Customers CustomersData_1[] = {
    /*BEGIN Customers DATA*/
    #include "../data/CustomersData.1.dat.h"
    /*END Customers DATA*/
};

// The included .dat.h file contains entries like:
{"36f3e1a5-...","John Doe","john@example.com"},
{"7b2c9d4f-...","Jane Smith","jane@example.com"},
// ...
```

Key components:
- `schema_generator.h/c`: Also generates empty data pages
- `generate_accessor_functions()`: Creates standard accessor functions for each page

### Accessor Functions

Each compiled data page exports standardized accessor functions:

```c
/**
 * @brief Returns the number of records in the page
 * @return Number of records
 */
int count(void) {
    return sizeof(CustomersData_1) / sizeof(Customers);
}

/**
 * @brief Returns a record at the specified position
 * @param pos Position of the record
 * @return Pointer to the record or NULL if out of bounds
 */
Customers* read(int pos) {
    if (pos < 0 || pos >= count()) {
        return NULL;
    }
    return &CustomersData_1[pos];
}
```

### Permission System

Umbra includes a permission system for access control:

```c
typedef enum {
    PERM_READ = 0x01,
    PERM_WRITE = 0x02,
    PERM_CREATE = 0x04,
    PERM_DROP = 0x08,
    PERM_ADMIN = 0xFF
} PermissionType;

typedef struct {
    char user[64];                 // Username
    char table[64];                // Table name
    PermissionType permissions;    // Permission flags
} UserPermission;
```

Key components:
- `permission.h/c`: Defines permission structures and operations




